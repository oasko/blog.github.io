---
title: 信息搜集之搜索
date: 2025-01-23 08:54:54
tags: 信息搜集
---
##


# tryhackme

## task2
评估信息时需要考虑的几点：

1.来源：确定发布信息的作者或组织。考虑他们是否在该主题上享有声誉和权威。发表博客文章并不代表某人在该主题上具有权威性。
2.证据和推理：检查主张是否有可靠的证据和逻辑推理。我们寻求确凿的事实和有力的论据。
3.客观性和偏见性：评估信息是否公正合理地呈现，是否反映了多种观点。我们不关心作者推动阴暗议程，无论是为了推销产品还是攻击竞争对手。
4.确证和一致性：通过多个独立来源的确证来验证所呈现的信息。检查多个可靠且信誉良好的来源是否同意核心主张。

![task](/images/信息搜集之搜索/屏幕截图%202025-01-23%20090909.png)
答案

snake oli

ss


## task3 
第三部分其实讲的是搜索引擎 介绍了些搜索运算符 我补充点 

### 基础运算符
“精确匹配”
使用双引号可以搜索与引号中的关键字或短语完全匹配的内容。例如：
"数字营销策略"：仅返回包含该完整短语的页面。

AND（逻辑与）
在两个搜索词之间使用 AND，可以确保结果中同时包含这两个词。例如：
SEO AND 优化：返回同时包含“SEO”和“优化”的页面。

OR（逻辑或）
使用 OR 或竖线 | 可以搜索包含其中一个或多个词的结果。例如：
SEO OR "搜索引擎优化"：返回包含“SEO”或“搜索引擎优化”的页面。

-（排除）
在搜索词前加 - 可以排除特定词。例如：
SEO -营销：返回包含“SEO”但不包含“营销”的页面。

*（通配符）
使用 * 可以匹配任意单词或短语。例如：
数字 * 策略：返回包含“数字”和“策略”之间任意单词的页面。

### 高级搜索运算符
site:
限制搜索结果来自特定网站。例如：
site:example.com SEO：仅在 example.com 网站内搜索包含“SEO”的页面。

intitle:
查找标题中包含特定词的页面。例如：
intitle:SEO：返回标题中包含“SEO”的页面。

inurl:
查找 URL 中包含特定词的页面。例如：
inurl:SEO：返回 URL 中包含“SEO”的页面。

filetype:
搜索特定文件类型的文档。例如：
filetype:pdf SEO：返回包含“SEO”的 PDF 文件。

related:
查找与某个网站相关的其他网站。例如：
related:example.com：返回与 example.com 相关的网站。

allintitle:
查找标题中包含所有指定词的页面。例如：
allintitle:SEO 策略：返回标题中同时包含“SEO”和“策略”的页面。

![task3](/images/信息搜集之搜索/task3.png)

第一题 我们就可以这么搜 filetype:pdf cyber warfare report

第二题 ss(socket statistics)

## task4
接下来轮到专业搜索引擎 信息收集主要也是靠这些
比如说shodan 

Virus Total是一个在线网站，使用多种防病毒引擎提供文件病毒扫描服务。它允许用户上传文件或提供 URL，以便通过单一操作使用多种防病毒引擎和网站扫描程序对其进行扫描。他们甚至可以输入文件哈希来检查之前上传的文件的结果。

ensys 专注于与互联网连接的主机、网站、证书和其他互联网资产。    

![task4](/images/信息搜集之搜索/task4.png)

我们在shodan中搜lghttpd就可以知道是us

第二天我们把hash值粘贴进Virus Total就能知道是啥

## task5
漏洞 可以到cnnvd国家信息安全漏洞库 cnvd 国家信息安全漏洞共享平台里面查看

exploit-db 漏洞数据库列出了来自不同作者的漏洞代码；其中一些漏洞代码已经过测试并标记为已验证

还有nist可以去康康

题目
![](/images/信息搜集之搜索/task5.png)

我们可以在nist里面搜一下这个编号 看描述知道是来自xz的


## task6
技术文档 有时候东西解不出来 是去看看官方的文档 往往在里面就有解决方案或者一些一下灵感

![task6](/source/images/信息搜集之搜索/task6.png)






## task 7
就是社交媒体 有点社工学的感觉了 从社交媒体的蛛丝马迹中寻找

![task7](/source/images/信息搜集之搜索/task7.png)

第一问这个东东叫领英 好像是这么翻译的 就是比较严肃点的社交媒体

第二问 Facebook就是日常风的社交媒体


# 正事时间
这个题目是做完了 信息收集可不止这么点 咱来细细盘点一下。

信息收集通常分为
- 域名信息收集
  
- 子域名信息收集

- 站点信息收集

- 敏感信息收集

- 服务器信息收集

- 端口信息收集

- 真实IP地址识别

- 社会工程学


## 域名信息收集
域名信息收集
**域名**（英语：Domain Name），又称**网域**，是由一串用**点分隔**的名字组成的Internet上某**一台计算机**或**计算机组**的名称，用于在数据传输时对计算机的**定位标识**（有时也指地理位置）。由于IP地址具有**不方便记忆**并且**不能显示地址组织**的**名称**和**性质**等缺点，人们设计出了域名，并通过网域名称系统来将域名和IP地址**相互映射**，使人更方便地访问互联网，而不用去记住能够被机器直接读取的IP地址数串。

### 分类
#### 顶级域名/一级域名：

顶级域（或顶级域名，也称为一级域名），是互联网DNS等级之中的最高级的域，它保存于DNS根域的名字空间中。顶级域名是域名的最后一个部分，即是域名最后一点之后的字母，例如在http://www.example.com这个域名中，顶级域是.com。

#### 二级域名：

除了顶级域名，还有二级域名，就是最靠近顶级域名左侧的字段。例如在http://www.example.com这个域名中，example就是二级域名。

#### 子域名：

子域名，在域名系统等级中，属于更高一层域的域。比如，mail.example.com和calendar.example.com是example.com的两个子域，而example.com则是顶级域.com的子域。凡顶级域名前加前缀的都是该顶级域名的子域名，而子域名根据技术的多少分为二级子域名，三级子域名以及多级子域名。

为什么要收集子域名

子域名枚举可以在**测试范围**内发现更多的**域**或**子域**，这将**增大**漏洞发现的**几率**。

有些**隐藏的**、**被忽略**的**子域**上运行的应用程序可能帮助我们发现**重大漏洞**。

在同一个组织的不同域或应用程序中往往存在相同的漏洞

假设我们的目标网络规模比较大，直接从主域入手显然是很不理智的，因为对于这种规模的目标，一般其**主域**都是**重点防护区域**，所以不如**先进入目标的某个子域**，然后再想办法迂回接近真正的目标，这无疑是个比较好的选择。

##### 利用在线工具查询
网上有很多子域名的查询站点，可通过它们检索某个给定域名的子域名。如：

- DNSdumpster：https://dnsdumpster.com/
- 
- whois反查：http://whois.chinaz.com/
- 
- virustotal：www.virustotal.com
- 
- 子域名爆破：https://phpinfo.me/domain/
- 
- IP反查绑定域名：http://dns.aizhan.com/
  
- https://hackertarget.com/
  
- find-dns-host-records/
  

- https://site.ip138.com


##### 通过证书透明度公开日志枚举子域名
证书透明度是证书授权机构的一个项目，证书授权机构会将每个SSL/TLS证书发布到公共日志中。一个SSL/TLS证书通常包含域名、子域名和邮件地址，这些也经常成为攻击者非常希望获得的有用信息。

查找某个域名所属证书的最简单的方法就是使用搜索引擎来搜索一些公开的CT日志，例如以下网站：

crt.sh：https://crt.sh
censys：https://censys.io

##### kali的工具
在kali中的信息收集模块的DNS分析中，有很多工具可以进行域名信息收集

- Dnsenum：域名信息收集
- Dnsmap：收集信息和枚举DNS信息
- Dnsrecon：用于DNS侦察
- Fierce ：子域名查询

##### windows的工具
Windows上的子域名查询工具主要由：

Layer子域名挖掘机
subDomainsbrute
K8
Sublist3r
Maltego

### Whois 查询
whois 是用来查询域名的**IP**以及**所有者**等信息的传输协议。简单说，whois就是一个用来**查询域名**是否已经被注册，以及**注册域名**的**详细信息**的**数据库**（如域名所有人、域名注册商），不同域名后缀的Whois信息需要到**不同**的Whois数据库查询。通过whois来实现对域名信息的查询，可以得到注册人的**姓名**和邮**箱信息**通常对测试个人站点非常有用，因为我们可以通过搜索引擎和社交网络挖掘出域名所有人的很多信息。

#### 在线查询

如今网上出现了一些网页接口简化的线上查询工具，可以一次向不同的数据库查询。网页接口的查询工具仍然依赖whois协议向服务器发送查询请求，命令列接口的工具仍然被系统管理员广泛使用。whois通常使用TCP协议43端口。每个域名/IP的whois信息由对应的管理机构保存。

常见的网站包括：

- Whois站长之家查询：http://whois.chinaz.com/

- 阿里云中国万网查询：https://whois.aliyun.com/

- Whois Lookup 查找目标网站所有者的信息：http://whois.domaintools.com/

- Netcraft Site Report 显示目标网站上使用的技术：http://toolbar.netcraft.com/site_report?url=

- Robtex DNS 查询显示关于目标网站的全面的DNS信息：https://www.robtex.com/

- 全球Whois查询：https://www.whois365.com/cn/

- 站长工具爱站查询：https://whois.aizhan.com/

还可以在Kali Linux下自带的Whois查询工具，通过命令Whois查询域名信息，只需输入要查询的域名即可

#### 备案信息查询

**网站备案信息**是根据**国家法律法规规定**，由**网站所有者**向国家**有关部门**申请的**备案**，是国家信息产业部对网站的一种管理途径，是为了防止在网上从事非法网站经营活动，当然主要是针对国内网站。

在备案查询中我们主要关注的是：单位信息例如**名称**、**备案编号**、**网站负责人**、**法人**、**电子邮箱**、**联系电话**等。

常用的备案信息查询网站有以下几个：

ICP/IP地址/域名信息备案管理系统：http://beian.miit.gov.cn/publish/query/indexFirst.action

ICP备案查询网：http://www.beianbeian.com/

备案吧吧：https://www.beian88.com/

天眼查：https://www.tianyancha.com/

## 站点信息收集
接下来进行web网站站点信息收集，主要收集如下信息：

- CMS指纹识别
- 历史漏洞
- 脚本语言
- 敏感目录/文件
- Waf识别

### CMS指纹识别
**CMS**（**内容管理系统**）又称为**整站系统**或**文章系统**，用于**网站内容管理**。用户只需要下载对应的CMS软件包，就能**部署搭建**，并**直接利用**CMS。但是各种CMS都具有其**独特的结构命名规则**和**特定的文件内容**，因此可以利用这些内容来获取**CMS站点**的**具体软件CMS**与**版本**。

在渗透测试中，对进行指纹识别是相当有必要的，识别出相应的CMS，才能查找与其相关的漏洞，然后才能进行相应的渗透操作。

常见的CMS有Dedecms(织梦)、Discuz、PHPWEB、PHPWind、PHPCMS、ECShop、Dvbbs、SiteWeaver、ASPCMS、帝国、Z-Blog、WordPress等。

（1）在线识别

如今，网上一些在线的网站查询CMS指纹识别，如下所示：

- BugScaner: http://whatweb.bugscaner.com/look/
- 潮汐指纹：http://finger.tidesec.net/
- 云悉：http://www.yunsee.cn/info.html
- WhatWeb: https://whatweb.net/
- 云悉指纹: http://www.yunsee.cn/finger.html

（2）利用工具

常见的CMS指纹识别工具有**WhatWeb**、**WebRobo**、**椰树**、**御剑Web指纹识别**。大禹CMS识别程序等，可以快速识别一些主流CMS。

当我们得知了一个站点的cms类型后，我们可以在网上查找与其相关的漏洞并进行相应的测试。

（3）手工识别

1. 根据HTTP响应头判断，重点关注**X-Powered-By**、**cookie**等字段
2. 根据HTML 特征，重点关注 **body**、**title**、**meta**等标签的内容和属性。
3. 根据特殊的t**class**判断。HTML 中存在特定 class 属性的某些 **div 标签**，如<body class="ke-content">

### 敏感目录/文件收集
也就是对**目标网站**做个**目录扫描**。在web渗透中，探测Web目录结构和隐藏的敏感文件是一个十分重要的环节，从中可以获取网站的**后台管理页面**、**文件上传界面**、**robots.txt**，甚至可能扫描出**备份文件**从而得到网站的**源代码**。

常见的网站目录的扫描工具主要有：

- 御剑后台扫描工具
- dirbuster扫描工具
- dirsearch扫描工具
- dirb
- wwwscan
- Sensitivefilescan

### Waf识别
**Web应用防护系统**（也称为：网站应用级入侵防御系统。英文：Web Application Firewall，简称： WAF）。利用国际上公认的一种说法：Web应用防火墙是通过执行一系列针对**HTTP/HTTPS**的安全策略来专门为Web应用提供保护的一款产品。

**wafw00f**是一个Web应用防火墙（WAF）指纹识别的工具。

#### wafw00f的工作原理：

1. 发送**正常**的**HTTP请求**，然后**分析响应**，这可以识别出很多WAF。

2. 如果不成功，它会发送一些（可能是恶意的）HTTP请求，使用简单的**逻辑推断**是哪一个WAF。

3. 如果这也不成功，它会分析之前返回的响应，使用**其它简单的算法猜测**是否有某个WAF或者安全解决方案响应了我们的攻击。


### 敏感信息收集
有时候，针对某些安全做得很好的目标，直接通过技术层面是无法完成渗透测试的。此时，便可以利用**搜索引擎搜索**目标**暴露在互联网上**的**关联信息**。例如：**数据库文件**、**SQL注入**、**服务配置信息**，甚至是通过Git找到**站点**泄露源代码，以及**Redis**等未授权访问、**Robots.txt**等敏感信息，从而达到渗透目的。

Google hacking
- **intext**：寻找正文中含有关键字的网页
- **intitle**：寻找标题中含有关键字的网页
- **allintitle**：用法和intitle类似，只不过可以指定多个词
- **inurl**：搜索url中含有关键词的网页
- **allinurl**：用法和inurl类似，只不过可以指定多个词
- **site**：指定访问的站点
- **filetype**：指定访问的文件类型
- **link**：指定链接的网页
- **related**：搜索相似类型的网页
- **info**：返回站点的指定信息，例如：info:www.baidu.- com   将返回百度的一些信息
- **phonebook**：电话簿查询美国街道地址和电话号码信息
- **Index of**：利用 Index of 语法可以发现允许目录浏览的web网站，就像在本地的普通目录一样


## Github信息泄露
itHub作为开源代码平台，给程序员提供了很多便利，但如果使用不当，比如将包含了账号密码、密钥等配置文件的代码上传了，导致攻击者能发现并进一步利用这些泄露的信息，就是一个典型的GitHub敏感信息泄露漏洞，再如开发人员在开发时，常常会先把源码提交到github，最后再从远程托管网站把源码pull到服务器的web目录下，如果忘记把.git文件删除，就造成此漏洞。利用.git文件恢复网站的源码，而源码里可能会有数据库的信息，

## 服务器信息收集
我们还需要对目标服务器的信息进行收集，主要包括一下部分：

- Web服务器指纹识别
- 真实IP地址识别
- 编程语言
- Web中间件
- 端口信息收集
- 后端存储技术识别

Web服务器指纹识别
Web服务器指纹识别是了解正在运行的web服务器类型和版本，目前市场上存在几种不同的web服务器提供商和软件版本，了解被测试的web服务器的类型，能让测试者更好去测试已知漏洞和大概的利用方法，将会在渗透测试过程中有很大的帮助，甚至会改变测试的路线。

### Web服务器指纹识别主要识别一下信息：

1、Web服务器**名称**，**版本**

2、Web服务器**后端**是否有**应用服务器**

3、**数据库**(DBMS)是否**部署在同一主机(host)**，数据库类型

4、Web应用使用的**编程语言**

5、Web**应用框架**

#### 手工检测

1. HTTP头分析
即查看HTTP响应头中的**Server**、**X-Powered-By**、**Cookie** 等字段，这也是最基本的方法。

2. 2. 协议行为
即从**HTTP头字段顺序分析**，观察HTTP响应头的**组织顺序**，因为每个服务器都有一个内部的H**TTP头排序**方法。

3. 浏览并观察网站
我们可以观察网站某些位置的**HTML源码**(**特殊的class名称**)及其**注释(comment)部分**，可能暴露有价值信息。观察**网站页面后缀**可以判断Web应用使用的编程语言和框架。

4. 刻意构造错误
错误页面可以给你提供关于服务器的大量信息。可以通过构造含有随机字符串的URL，并访问它来尝试得到404页面。


#### 利用工具识别

**whatweb**是一款用于辅助的自动化Web应用指纹分析工具

## 真实IP地址识别
在渗透测试中，一般只会给你一个域名，那么我们就要根据这个域名来确定目标服务器的真实IP，我们可以通过像www.ip138.com这样的IP查询网直接获取目标的一些IP及域名信息，但这里的前提是目标服务器**没有使用CDN**

### 什么是CDN

CDN的全称是Content Delivery Network，即内容分发网络。

我们也可以设置代理或者通过在线ping网站来在不同地区进行ping测试，然后对比每个地区ping出的IP结果，查看这些IP是否一致，**一致**，则极有可能**不存在CDN**。根据 CDN 的工作原理，如果网站使用了 CDN，那么从**全国各地访问网站**的 IP 地址是**各个 CDN 节点的 IP 地址**，那么如果ping出来的IP**大多不太一样**或者**规律性很强**，可以尝试查询这些IP的归属地，判断是否存在CDN。有以下网站可以进行ping测试：
```
 http://ping.chinaz.com/
 https://www.wepcc.com
 https://www.17ce.com
```

### 如何绕过CDN找到目标真实IP？

1. 利用子域名。一般来说很多站长可能只会对主站或者流量较大的分站使用CDN，但是一些流量比较小的分站可能没有挂CDN，这些分站和主站虽然不是同一个IP但是都在同一个C段下面的情况，所以我们可以通过ping二级域名获取分站lP，从而能判断出目标的真实IP段。

2. 查询主域。以前用CDN的时候有个习惯，只让WWW域名使用cdn，秃域名不使用，为的是在维护网站时更方便，不用等cdn缓存。所以试着把目标网站的www去掉，ping一下看ip是不是变了

3. 扫描网站敏感文件，如phpinfo.php等，从而找到目标的真实IP。

4. 从国外访问。国内很多CDN厂商因为各种原因只做了国内的线路，而针对国外的线路可能几乎没有，此时我们使用国外的主机直接访问可能就能获取到真实P。我们可以通过国外在线代理网站访问，可能会得到真实的IP地址，

https://asm.ca.com/en/ping.php

5. 查看域名历史解析记录。也许目标很久之前没有使用CDN，所以可能会存在使用 CDN 前的记录。所以可以通过https://www.netcraft.com、https://viewdns.info/等网站来观察域名的IP历史记录。

6. Nslookup查询。查询域名的NS记录、MX记录、TXT记录等很有可能指向的是真实ip或同C段服务器。

7. 利用网络空间搜索引擎。这里主要是利用网站返回的内容寻找真实原始IP，如果原始服务器IP也返回了网站的内容，那么可以在网上搜索大量的相关数据。最常见的网络空间搜索引擎有如下：

- Shodan：https://www.shodan.io/
- 钟馗之眼：https://www.zoomeye.org/
- FOFA：https://fofa.so/

8. 利用网站漏洞。比如有代码执行漏洞、SSRF、存储型的XSS都可以让服务器主动访问我们预设的web服务器，那么就能在日志里面看见目标网站服务器的真实IP。

## 端口信息收集
1. nmap工具扫描

2. 由于使用工具通常会在目标网站留下痕迹，接下来提供一种在线网站探测方法。

- 在线网站：http://tool.chinaz.com/port/
- ThreatScan在线网站（网站基础信息收集）：https://scan.top15.cn/
- Shodan：https://www.shodan.io/


